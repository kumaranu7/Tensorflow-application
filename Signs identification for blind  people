def one_hot_matrix(labels, C):
    C = tf.constant(C, name ='C')
    one_hot_matrix = tf.one_hot(indices=labels, depth = C, axis = 0)
    sess = tf.Session()
    one_hot = sess.run(one_hot_matrix)
    sess.close()
    return one_hot
    
def create_placeholder(n_x, n_y):
X = tf.placeholder(float32, shape = [n_x, None])
Y = tf.placeholder(float32, shape = [n_y, None])
return X, Y

def initialize_parameters():
    W1 = tf.get_variable('W1', [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))
    b2 = tf.get_variable('b2', [25, 1], initializer = tf.zeros_initializer())
    W2 = tf.get_variable('W2', [12, 25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))
    b2 = tf.get_variable('b2', [12, 1], initializer = tf.zeros_initializer())
    W3 = tf.get_variable('W3', [6, 12], initializer = tf.contrib.layers.xavier_initializer(seed=1))
    b3 = tf.get_variable('b3', [6, 1], initializer = tf.zeros_initializer())
    
    parameters = {
        'W1':W1,
        'b1':b1,
        'W2':W2,
        'b2':b2,
        'W3':W3,
        'b3' : b3
    }
    return parameters

  def forward_propagation(X, parameters):
    Z1 = tf.sum(tf.matmul(W1, X), b1)
    A1 = tf.nn.relu(Z1)
    Z2 = tf.sum(tf.matmul(W2, X), b2)
    A2 = tf.nn.relu(Z2)
    Z3 = tf.sum(tf.matmul(W3, X), b3)
    return Z3
   
   def compute_cost(Z3, Y):
    logits = tf.transpose(Z3)
    labels = tf.tranpose(Y)
    cost = tf.reduce.mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels =  Y))
    return cost
    
    #Processing data
X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1)
y_train_flatten = y_train_orig.reshape(y_train_orig.shape[0], -1)
#normalizing the data
X_train = X_train_flatten/255
y_train = y_train_flatten/255
y_train = convert_to_one_hot(y_train_orig, 6)
y_test = convert_to_one_hot(y_test_orig, 6)

#creating model

def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.001, num_epoch = 1500, minibatch_size = 32, print_cost = True):
   
    ops.reset_default_graph()   # to be able to rerun the model without overwriting tf variables
    tf.set_random_seed(1)
    seed = 5
    (n_x, m) = X_train.shape
    costs = []
    
    X, Y = create_placeholder(n_x, n_y)
    parameters = initializer_parameters()
    Z3 = forward_propagation(X, parameters)
    cost = compute_cost()
    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)
    
    #initializing tf session
    init = tf.global_variable_initializer()
    with tf.Session() as sess:
        sess.run(init)
        for epoch in range(num_epoch):
            epoch_cost = 0
            num_minibatch = int(m/minibatch_size)
            seed = seed+1
            minibatches = random_mini_batches(X_train, y_train, minibatch_size, seed)
            
            for minibatch in minibatches:
                (minibatch_X, minibatch_y) = minibatch
                _, minibatch_cost = sess.run([optimizer, cost], feed_dict = {X: minibatch_X, Y: minibatch_y})
                epoch_cost += minibatch_cost/num_minibatch
            
            if print_cost == True and epoch%100 ==0 :
                print('Cost after epoch %i: %f' %(epoch, epoch_cost))
            if print_cost == True and epoch %5==0:
                costs.append(epoch_cost)
            
        plt.plot(np.squeeze(costs))
        plt.ylabel('cost')
        plt.xlabel('iterations (per fives)')
        plt.title('Learning rate=' + str(learning_rate) )
        plt.show()
        
        parameters = sess.run(parameters)
        print ("Parameters have been trained!")
        
        correct_predictions = tf.equal(tf.argmax(Z3), tf.argmax(Y))
        accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'))
        
        print('Train accuracy', accuracy.eval({X:X_train, y:y_train}))
        print('Test accuracy', accuracy.eval({X:X_test, y:y_test}))
        
        return parameters
